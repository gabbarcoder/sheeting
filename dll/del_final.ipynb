{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def custom_tanh(x):\n",
    "    return (K.exp(x) - K.exp(-x)) / (K.exp(x) + K.exp(-x))\n",
    "def custom_relu(x):\n",
    "    return K.maximum(0.0, x)\n",
    "def custom_sigmoid(x):\n",
    "    return 1 / (1 + K.exp(-x))\n",
    "\n",
    "\n",
    "model = Sequential([ Dense(64, input_shape=(10,)), \n",
    "    Activation(custom_relu), Dense(64), Activation(custom_tanh), Dense(1), Activation(custom_sigmoid) ])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "X_train = np.random.random((100, 10))  \n",
    "Y_train = np.random.randint(2, size=(100, 1))  \n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=10)\n",
    "predictions = model.predict(X_train[:5])\n",
    "\n",
    "\n",
    "print(\"Predictions:\\n\", predictions)\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "accuracy = accuracy_score(Y_train[:5], binary_predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "# Precision\n",
    "precision = precision_score(Y_train[:5], binary_predictions)\n",
    "print(\"Precision:\", precision)\n",
    "# Recall\n",
    "recall = recall_score(Y_train[:5], binary_predictions)\n",
    "print(\"Recall:\", recall)\n",
    "# F1-score\n",
    "f1 = f1_score(Y_train[:5], binary_predictions)\n",
    "print(\"F1-score:\", f1)\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(Y_train[:5], binary_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def train_model(optimizer):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=0)\n",
    "    return history.history['accuracy'][-1] * 100\n",
    "accuracy_gd = train_model('sgd')\n",
    "accuracy_sgd = train_model('adam')\n",
    "\n",
    "print(\"Gradient Descent Accuracy: {:.2f}%\".format(accuracy_gd))\n",
    "print(\"Stochastic Gradient Descent Accuracy: {:.2f}%\".format(accuracy_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define user-defined activation function\n",
    "def custom_activation(x):\n",
    "    return tf.nn.leaky_relu(x, alpha=0.2)\n",
    "\n",
    "# Function to create and train the model with given dropout probability or clipout value\n",
    "def train_model(activation_type, value):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),  # Flatten the input\n",
    "        tf.keras.layers.Dense(64, activation=custom_activation),\n",
    "        tf.keras.layers.Dense(64, activation=custom_activation),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    if activation_type == 'dropout':\n",
    "        model.add(tf.keras.layers.Dropout(value))\n",
    "        model.add(tf.keras.layers.Dropout(value))\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    elif activation_type == 'clipout':\n",
    "        opt = tf.keras.optimizers.Adam(clipvalue=value)  # Gradient clipping\n",
    "        model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=64, verbose=0)\n",
    "    _, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return test_accuracy\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define a list of dropout probabilities to test\n",
    "dropout_probs = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# Define a list of clipout values to test\n",
    "clipout_values = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Initialize lists to store accuracies\n",
    "dropout_accuracies = []\n",
    "clipout_accuracies = []\n",
    "\n",
    "# Test dropout probabilities\n",
    "for dropout_prob in dropout_probs:\n",
    "    accuracy = train_model('dropout', dropout_prob)\n",
    "    dropout_accuracies.append(accuracy)\n",
    "    print(\"Dropout Probability:\", dropout_prob, \"- Test Accuracy:\", accuracy)\n",
    "\n",
    "# Test clipout values\n",
    "for clipout_value in clipout_values:\n",
    "    accuracy = train_model('clipout', clipout_value)\n",
    "    clipout_accuracies.append(accuracy)\n",
    "    print(\"Clipout Value:\", clipout_value, \"- Test Accuracy:\", accuracy)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(dropout_probs, dropout_accuracies, marker='o', label='Dropout')\n",
    "plt.plot(clipout_values, clipout_accuracies, marker='s', label='Clipout')\n",
    "plt.xlabel('Probability/Value')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Effect of Dropout and Clipout on Test Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "x_task1 = np.random.rand(1000, 10)\n",
    "x_task2 = np.random.rand(1000, 10)  \n",
    "y_task1 = np.random.rand(1000, 1) \n",
    "y_task2 = np.random.rand(1000, 1) \n",
    " \n",
    "input_layer = Input(shape=(10,))\n",
    "\n",
    "branch_task1 = Dense(64, activation='relu')(input_layer)\n",
    "output_task1 = Dense(1, name='output_task1')(branch_task1)\n",
    "\n",
    "branch_task2 = Dense(64, activation='relu')(input_layer)\n",
    "output_task2 = Dense(1, name='output_task2')(branch_task2)\n",
    "\n",
    "model_task1 = Model(inputs=input_layer, outputs=output_task1)\n",
    "model_task2 = Model(inputs=input_layer, outputs=output_task2)\n",
    "\n",
    "model_task1.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "model_task2.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "multitask_model = Model(inputs=input_layer, outputs=[output_task1, output_task2])\n",
    "\n",
    "multitask_model.compile(optimizer='adam',\n",
    "                        loss={'output_task1': 'mean_squared_error',\n",
    "                              'output_task2': 'mean_squared_error'},\n",
    "                        loss_weights={'output_task1': 0.5, 'output_task2': 0.5},\n",
    "                        metrics={'output_task1': 'mae', 'output_task2': 'mae'})\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = multitask_model.fit(x_task1, [y_task1, y_task2], validation_split=0.2, epochs=50, callbacks=[early_stopping])\n",
    "\n",
    "# Plotting Training and Validation Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Task 1 Training Loss')\n",
    "plt.plot(history.history['val_output_task1_loss'], label='Task 1 Validation Loss')\n",
    "plt.plot(history.history['output_task2_loss'], label='Task 2 Training Loss')\n",
    "plt.plot(history.history['val_output_task2_loss'], label='Task 2 Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting Early Stopping\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(early_stopping.stopped_epoch, history.history['val_loss'][early_stopping.stopped_epoch], 'ro', label='Stopped Epoch')\n",
    "plt.title('Early Stopping')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(max_features, 32),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=128, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "def predict_sentiment(text, model, maxlen=maxlen):\n",
    "    tokens = imdb.get_word_index()\n",
    "    tokens = {k: (v + 3) for k, v in tokens.items()}\n",
    "    tokens[\"<PAD>\"] = 0\n",
    "    tokens[\"<START>\"] = 1\n",
    "    tokens[\"<UNK>\"] = 2\n",
    "    tokens[\"<UNUSED>\"] = 3\n",
    "    \n",
    "    reverse_tokens = dict([(value, key) for (key, value) in tokens.items()])\n",
    "    \n",
    "    sequence = [tokens.get(word, tokens[\"<UNK>\"]) for word in text.split()]\n",
    "    sequence = pad_sequences([sequence], maxlen=maxlen)\n",
    "    \n",
    "    prediction = model.predict(sequence)[0, 0]\n",
    "    sentiment = \"positive\" if prediction >= 0.5 else \"negative\"\n",
    "    return sentiment, prediction\n",
    "\n",
    "#text1 = \"This movie was great! I really enjoyed it.\"\n",
    "#text2 = \"The acting was terrible and the plot was boring.\"\n",
    "\n",
    "\n",
    "\n",
    "text1= input(\" ENTER YOUR TEXT : \")\n",
    "text2= input (\"ENTER SECOND TEXT : \")\n",
    "\n",
    "sentiment1, score1 = predict_sentiment(text1, model)\n",
    "sentiment2, score2 = predict_sentiment(text2, model)\n",
    "\n",
    "print(f\"Text: '{text1}' | Sentiment: {sentiment1} | Score: {score1:.4f}\")\n",
    "print(f\"Text: '{text2}' | Sentiment: {sentiment2} | Score: {score2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import datetime\n",
    "\n",
    "ticker = 'AAPL'\n",
    "data = yf.download(ticker, start='2010-01-01', end=datetime.datetime.now().strftime('%Y-%m-%d'))['Close']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))\n",
    "\n",
    "time_step = 60\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data, test_data = scaled_data[:train_size], scaled_data[train_size:]\n",
    "\n",
    "X_train, y_train = [], []\n",
    "for i in range(time_step, len(train_data)):\n",
    "    X_train.append(train_data[i-time_step:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "X_test, y_test = [], []\n",
    "for i in range(time_step, len(test_data)):\n",
    "    X_test.append(test_data[i-time_step:i, 0])\n",
    "    y_test.append(test_data[i, 0])\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(time_step, 1)),\n",
    "    LSTM(50),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=1, validation_data=(X_test, y_test))\n",
    "\n",
    "train_predict = scaler.inverse_transform(model.predict(X_train))\n",
    "test_predict = scaler.inverse_transform(model.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(data.index, data.values, label='Actual Stock Price')\n",
    "plt.plot(data.index[time_step:train_size], train_predict, label='Training Predictions')\n",
    "plt.plot(data.index[train_size+time_step:], test_predict, label='Testing Predictions')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.plot(history.history['loss'], label='Training Loss', color='tab:blue')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', color='tab:orange')\n",
    "ax1.tick_params(axis='y')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.plot(history.history['mae'], label='Training MAE', color='tab:green')\n",
    "ax2.plot(history.history['val_mae'], label='Validation MAE', color='tab:red')\n",
    "ax2.tick_params(axis='y')\n",
    "fig.tight_layout()\n",
    "fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "plt.title('Model Loss and MAE')\n",
    "plt.show()\n",
    "\n",
    "def predict_next_day(model, data, time_step):\n",
    "    last_data = scaler.transform(data[-time_step:].values.reshape(-1, 1))\n",
    "    next_day_prediction = model.predict(last_data.reshape(1, time_step, 1))\n",
    "    return scaler.inverse_transform(next_day_prediction)[0, 0]\n",
    "\n",
    "next_day_price = predict_next_day(model, data, time_step)\n",
    "print(f'Next day predicted stock price: {next_day_price}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "import datetime\n",
    "\n",
    "ticker = 'AAPL'\n",
    "data = yf.download(ticker, start='2010-01-01', end=datetime.datetime.now().strftime('%Y-%m-%d'))['Close']\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))\n",
    "\n",
    "time_step = 60\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data, test_data = scaled_data[:train_size], scaled_data[train_size:]\n",
    "\n",
    "def create_sequences(data, time_step):\n",
    "    X, y = [], []\n",
    "    for i in range(time_step, len(data)):\n",
    "        X.append(data[i-time_step:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_sequences(train_data, time_step)\n",
    "X_test, y_test = create_sequences(test_data, time_step)\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(50, return_sequences=True, input_shape=(time_step, 1)),\n",
    "    GRU(50),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=1, validation_data=(X_test, y_test))\n",
    "\n",
    "train_predict = scaler.inverse_transform(model.predict(X_train))\n",
    "test_predict = scaler.inverse_transform(model.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(data.index, data.values, label='Actual Stock Price')\n",
    "plt.plot(data.index[time_step:train_size], train_predict, label='Training Predictions')\n",
    "plt.plot(data.index[train_size + time_step:], test_predict, label='Testing Predictions')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.plot(history.history['loss'], label='Training Loss', color='tab:blue')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', color='tab:orange')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.plot(history.history['mae'], label='Training MAE', color='tab:green')\n",
    "ax2.plot(history.history['val_mae'], label='Validation MAE', color='tab:red')\n",
    "fig.tight_layout()\n",
    "fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "plt.title('Model Loss and MAE')\n",
    "plt.show()\n",
    "\n",
    "def predict_next_day(model, data, time_step):\n",
    "    last_data = scaler.transform(data[-time_step:].values.reshape(-1, 1))\n",
    "    next_day_prediction = model.predict(last_data.reshape(1, time_step, 1))\n",
    "    return scaler.inverse_transform(next_day_prediction)[0, 0]\n",
    "\n",
    "next_day_price = predict_next_day(model, data, time_step)\n",
    "print(f'Next day predicted stock price: {next_day_price}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.plot(history.history['loss'], label='Training Loss', color='tab:blue')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', color='tab:orange')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy', color='tab:green')\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', color='tab:red')\n",
    "fig.tight_layout()\n",
    "fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "plt.title('Model Loss and Accuracy')\n",
    "plt.show()\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "num_predictions = 10\n",
    "print(\"Predictions with Images:\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(num_predictions):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Predicted: {predicted_labels[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train, y_train = x_train[:5000], y_train[:5000]\n",
    "x_test, y_test = x_test[:1000], y_test[:1000]\n",
    "\n",
    "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.int64)\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.int64)\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "def plot_loss(losses, title):\n",
    "    plt.plot(losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "def create_adversarial_pattern(model, input_image, input_label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_image)\n",
    "        prediction = model(input_image)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(input_label, prediction)\n",
    "    gradient = tape.gradient(loss, input_image)\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    return signed_grad, loss\n",
    "\n",
    "def adversarial_training(model, x_train, y_train, epochs=5, epsilon=0.1):\n",
    "    batch_size = 64\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            x_batch = x_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            perturbations, loss = create_adversarial_pattern(model, x_batch, y_batch)\n",
    "            epoch_loss += np.mean(loss)\n",
    "            x_adv = x_batch + epsilon * perturbations\n",
    "            x_adv = tf.clip_by_value(x_adv, 0, 1)\n",
    "            model.train_on_batch(x_adv, y_batch)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed\")\n",
    "        losses.append(epoch_loss/epochs)\n",
    "    plot_loss(losses, \"Adversarial Training Loss\")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
    "adversarial_training(model, x_train, y_train, epochs=5)\n",
    "tangent_vectors = tf.random.normal((28, 28))\n",
    "\n",
    "def tangent_propagation_loss(model, x, y, lambda_tangent=0.1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        predictions = model(x)\n",
    "        classification_loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)\n",
    "\n",
    "    gradients = tape.gradient(predictions, x)\n",
    "    tangent_loss = tf.reduce_sum(tf.square(tf.tensordot(gradients, tangent_vectors, axes=1)))\n",
    "\n",
    "    return classification_loss + lambda_tangent * tangent_loss\n",
    "\n",
    "\n",
    "def train_with_tangent_propagation(model, x_train, y_train, epochs=5, lambda_tangent=0.1):\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    batch_size = 64\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            x_batch = x_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = tangent_propagation_loss(model, x_batch, y_batch, lambda_tangent)\n",
    "            epoch_loss += np.mean(loss)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed\")\n",
    "        losses.append(epoch_loss/epochs)\n",
    "    plot_loss(losses, \"Tangent Propogation Loss\")\n",
    "\n",
    "train_with_tangent_propagation(model, x_train, y_train, epochs=5)\n",
    "\n",
    "def tangent_classifier(model, x_train, y_train, x_test, tangent_vectors):\n",
    "    x_train_flat = tf.reshape(x_train, (x_train.shape[0], -1))\n",
    "    x_test_flat = tf.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "    distances = tf.norm(x_test_flat[:, tf.newaxis, :] - x_train_flat[tf.newaxis, :, :], axis=2)\n",
    "    nearest_indices = tf.argmin(distances, axis=1)\n",
    "    predictions = tf.gather(y_train, nearest_indices)\n",
    "    return predictions.numpy()\n",
    "\n",
    "y_pred = tangent_classifier(model, x_train.numpy(), y_train.numpy(), x_test[:100].numpy(), tangent_vectors.numpy())\n",
    "print(\"Tangent Classifier Accuracy:\", np.mean(y_pred == y_test[:100].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class SimpleRBM:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.W = tf.Variable(tf.random.normal([input_size, output_size], 0.01))\n",
    "        self.h_bias = tf.Variable(tf.zeros([output_size]))\n",
    "        self.v_bias = tf.Variable(tf.zeros([input_size]))\n",
    "\n",
    "    def sample(self, probs):\n",
    "        return tf.nn.relu(tf.sign(probs - tf.random.uniform(tf.shape(probs))))\n",
    "\n",
    "    def step(self, v):\n",
    "        h_probs = tf.nn.sigmoid(tf.matmul(v, self.W) + self.h_bias)\n",
    "        h_sample = self.sample(h_probs)\n",
    "        v_probs = tf.nn.sigmoid(tf.matmul(h_sample, tf.transpose(self.W)) + self.v_bias)\n",
    "        v_sample = self.sample(v_probs)\n",
    "        return h_sample, v_sample\n",
    "\n",
    "    def train(self, data, epochs=1000, lr=0.1):\n",
    "        for epoch in range(epochs):\n",
    "            for v in data:\n",
    "                v = np.reshape(v, (1, -1))\n",
    "                h_sample, v_sample = self.step(v)\n",
    "                pos_grad = tf.matmul(tf.transpose(v), h_sample)\n",
    "                neg_grad = tf.matmul(tf.transpose(v_sample), self.step(v_sample)[0])\n",
    "                self.W.assign_add(lr * (pos_grad - neg_grad))\n",
    "                self.v_bias.assign_add(lr * tf.reduce_mean(v - v_sample, axis=0))\n",
    "                self.h_bias.assign_add(lr * tf.reduce_mean(h_sample - self.step(v_sample)[0], axis=0))\n",
    "\n",
    "    def transform(self, data):\n",
    "        data = np.reshape(data, (1, -1))\n",
    "        h_probs = tf.nn.sigmoid(tf.matmul(data, self.W) + self.h_bias)\n",
    "        return h_probs\n",
    "\n",
    "\n",
    "input_size = 6\n",
    "hidden_size_1 = 3\n",
    "hidden_size_2 = 2\n",
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "data = np.array([[1, 1, 1, 0, 0, 0],\n",
    "                 [1, 0, 1, 0, 0, 0],\n",
    "                 [1, 1, 1, 0, 0, 0],\n",
    "                 [0, 0, 1, 1, 1, 0],\n",
    "                 [0, 0, 1, 1, 0, 0],\n",
    "                 [0, 0, 1, 1, 1, 0]], dtype=np.float32)\n",
    "\n",
    "rbm1 = SimpleRBM(input_size, hidden_size_1)\n",
    "rbm1.train(data, epochs, learning_rate)\n",
    "h1 = np.array([rbm1.transform(v) for v in data])\n",
    "\n",
    "rbm2 = SimpleRBM(hidden_size_1, hidden_size_2)\n",
    "rbm2.train(h1, epochs, learning_rate)\n",
    "h2 = np.array([rbm2.transform(h) for h in h1])\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Features from RBM1:\\n\", h1)\n",
    "print(\"Features from RBM2:\\n\", h2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
