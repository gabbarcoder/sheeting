{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 01:27:55.750042: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-10 01:27:59.330941: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-10 01:27:59.330994: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-10 01:27:59.872082: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-10 01:28:00.388338: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-10 01:28:03.564484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 3s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 01:28:24.314537: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-10 01:28:32.491092: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-10 01:28:32.491312: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-10 01:28:32.492632: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-10 01:28:32.492754: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-10 01:28:32.492825: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-10 01:28:32.629857: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-10 01:28:32.630032: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-10 01:28:32.630119: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-10 01:28:32.630190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2243 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-06-10 01:28:33.183833: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 01:28:40.845813: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-10 01:28:45.848455: I external/local_xla/xla/service/service.cc:168] XLA service 0x7efb7ef5e150 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-10 01:28:45.848491: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2024-06-10 01:28:45.853724: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717963125.915749    7619 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 34s 36ms/step - loss: 0.4355 - accuracy: 0.7922 - val_loss: 0.3425 - val_accuracy: 0.8488\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.2571 - accuracy: 0.8985 - val_loss: 0.3688 - val_accuracy: 0.8422\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1801 - accuracy: 0.9331 - val_loss: 0.3979 - val_accuracy: 0.8374\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.1274 - accuracy: 0.9546 - val_loss: 0.4965 - val_accuracy: 0.8302\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1021 - accuracy: 0.9651 - val_loss: 0.5694 - val_accuracy: 0.8270\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5833 - accuracy: 0.8242\n",
      "Test Accuracy: 0.8241999745368958\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM, Dropout, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "max_words = 10000\n",
    "maxlen = 100\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "model = Sequential([\n",
    "    Embedding(max_words, 32, input_length=maxlen),\n",
    "    LSTM(32),  # Using LSTM instead of SimpleRNN\n",
    "    Dropout(0.2),  # Adding dropout regularization\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "test_text = input(\"Enter a string:\")\n",
    "test_words = test_text.lower().split()\n",
    "test_sequence = [[word_index[word] if word in word_index else 0 for word in test_words]]\n",
    "test_sequence = pad_sequences(test_sequence, maxlen=maxlen)\n",
    "prediction = model.predict(test_sequence)\n",
    "print(\"Prediction:\",prediction[0][0])\n",
    "\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import datetime\n",
    "\n",
    "ticker = 'AAPL'\n",
    "data = yf.download(ticker, start='2010-01-01', end=datetime.datetime.now().strftime('%Y-%m-%d'))['Close']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))\n",
    "\n",
    "time_step = 60\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data, test_data = scaled_data[:train_size], scaled_data[train_size:]\n",
    "\n",
    "X_train, y_train = [], []\n",
    "for i in range(time_step, len(train_data)):\n",
    "    X_train.append(train_data[i-time_step:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "X_test, y_test = [], []\n",
    "for i in range(time_step, len(test_data)):\n",
    "    X_test.append(test_data[i-time_step:i, 0])\n",
    "    y_test.append(test_data[i, 0])\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(time_step, 1)),\n",
    "    LSTM(50),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=1, validation_data=(X_test, y_test))\n",
    "\n",
    "train_predict = scaler.inverse_transform(model.predict(X_train))\n",
    "test_predict = scaler.inverse_transform(model.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(data.index, data.values, label='Actual Stock Price')\n",
    "plt.plot(data.index[time_step:train_size], train_predict, label='Training Predictions')\n",
    "plt.plot(data.index[train_size+time_step:], test_predict, label='Testing Predictions')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.plot(history.history['loss'], label='Training Loss', color='tab:blue')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', color='tab:orange')\n",
    "ax1.tick_params(axis='y')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.plot(history.history['mae'], label='Training MAE', color='tab:green')\n",
    "ax2.plot(history.history['val_mae'], label='Validation MAE', color='tab:red')\n",
    "ax2.tick_params(axis='y')\n",
    "fig.tight_layout()\n",
    "fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "plt.title('Model Loss and MAE')\n",
    "plt.show()\n",
    "\n",
    "def predict_next_day(model, data, time_step):\n",
    "    last_data = scaler.transform(data[-time_step:].values.reshape(-1, 1))\n",
    "    next_day_prediction = model.predict(last_data.reshape(1, time_step, 1))\n",
    "    return scaler.inverse_transform(next_day_prediction)[0, 0]\n",
    "\n",
    "next_day_price = predict_next_day(model, data, time_step)\n",
    "print(f'Next day predicted stock price: {next_day_price}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "import datetime\n",
    "\n",
    "ticker = 'AAPL'\n",
    "data = yf.download(ticker, start='2010-01-01', end=datetime.datetime.now().strftime('%Y-%m-%d'))['Close']\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))\n",
    "\n",
    "time_step = 60\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data, test_data = scaled_data[:train_size], scaled_data[train_size:]\n",
    "\n",
    "def create_sequences(data, time_step):\n",
    "    X, y = [], []\n",
    "    for i in range(time_step, len(data)):\n",
    "        X.append(data[i-time_step:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_sequences(train_data, time_step)\n",
    "X_test, y_test = create_sequences(test_data, time_step)\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(50, return_sequences=True, input_shape=(time_step, 1)),\n",
    "    GRU(50),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=1, validation_data=(X_test, y_test))\n",
    "\n",
    "train_predict = scaler.inverse_transform(model.predict(X_train))\n",
    "test_predict = scaler.inverse_transform(model.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(data.index, data.values, label='Actual Stock Price')\n",
    "plt.plot(data.index[time_step:train_size], train_predict, label='Training Predictions')\n",
    "plt.plot(data.index[train_size + time_step:], test_predict, label='Testing Predictions')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.plot(history.history['loss'], label='Training Loss', color='tab:blue')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', color='tab:orange')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.plot(history.history['mae'], label='Training MAE', color='tab:green')\n",
    "ax2.plot(history.history['val_mae'], label='Validation MAE', color='tab:red')\n",
    "fig.tight_layout()\n",
    "fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "plt.title('Model Loss and MAE')\n",
    "plt.show()\n",
    "\n",
    "def predict_next_day(model, data, time_step):\n",
    "    last_data = scaler.transform(data[-time_step:].values.reshape(-1, 1))\n",
    "    next_day_prediction = model.predict(last_data.reshape(1, time_step, 1))\n",
    "    return scaler.inverse_transform(next_day_prediction)[0, 0]\n",
    "\n",
    "next_day_price = predict_next_day(model, data, time_step)\n",
    "print(f'Next day predicted stock price: {next_day_price}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.plot(history.history['loss'], label='Training Loss', color='tab:blue')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', color='tab:orange')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy', color='tab:green')\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', color='tab:red')\n",
    "fig.tight_layout()\n",
    "fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "plt.title('Model Loss and Accuracy')\n",
    "plt.show()\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "num_predictions = 10\n",
    "print(\"Predictions with Images:\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(num_predictions):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Predicted: {predicted_labels[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train, y_train = x_train[:5000], y_train[:5000]\n",
    "x_test, y_test = x_test[:1000], y_test[:1000]\n",
    "\n",
    "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.int64)\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.int64)\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "def plot_loss(losses, title):\n",
    "    plt.plot(losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "def create_adversarial_pattern(model, input_image, input_label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_image)\n",
    "        prediction = model(input_image)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(input_label, prediction)\n",
    "    gradient = tape.gradient(loss, input_image)\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    return signed_grad, loss\n",
    "\n",
    "def adversarial_training(model, x_train, y_train, epochs=5, epsilon=0.1):\n",
    "    batch_size = 64\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            x_batch = x_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            perturbations, loss = create_adversarial_pattern(model, x_batch, y_batch)\n",
    "            epoch_loss += np.mean(loss)\n",
    "            x_adv = x_batch + epsilon * perturbations\n",
    "            x_adv = tf.clip_by_value(x_adv, 0, 1)\n",
    "            model.train_on_batch(x_adv, y_batch)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed\")\n",
    "        losses.append(epoch_loss/epochs)\n",
    "    plot_loss(losses, \"Adversarial Training Loss\")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
    "adversarial_training(model, x_train, y_train, epochs=5)\n",
    "tangent_vectors = tf.random.normal((28, 28))\n",
    "\n",
    "def tangent_propagation_loss(model, x, y, lambda_tangent=0.1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        predictions = model(x)\n",
    "        classification_loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)\n",
    "\n",
    "    gradients = tape.gradient(predictions, x)\n",
    "    tangent_loss = tf.reduce_sum(tf.square(tf.tensordot(gradients, tangent_vectors, axes=1)))\n",
    "\n",
    "    return classification_loss + lambda_tangent * tangent_loss\n",
    "\n",
    "\n",
    "def train_with_tangent_propagation(model, x_train, y_train, epochs=5, lambda_tangent=0.1):\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    batch_size = 64\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            x_batch = x_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = tangent_propagation_loss(model, x_batch, y_batch, lambda_tangent)\n",
    "            epoch_loss += np.mean(loss)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed\")\n",
    "        losses.append(epoch_loss/epochs)\n",
    "    plot_loss(losses, \"Tangent Propogation Loss\")\n",
    "\n",
    "train_with_tangent_propagation(model, x_train, y_train, epochs=5)\n",
    "\n",
    "def tangent_classifier(model, x_train, y_train, x_test, tangent_vectors):\n",
    "    x_train_flat = tf.reshape(x_train, (x_train.shape[0], -1))\n",
    "    x_test_flat = tf.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "    distances = tf.norm(x_test_flat[:, tf.newaxis, :] - x_train_flat[tf.newaxis, :, :], axis=2)\n",
    "    nearest_indices = tf.argmin(distances, axis=1)\n",
    "    predictions = tf.gather(y_train, nearest_indices)\n",
    "    return predictions.numpy()\n",
    "\n",
    "y_pred = tangent_classifier(model, x_train.numpy(), y_train.numpy(), x_test[:100].numpy(), tangent_vectors.numpy())\n",
    "print(\"Tangent Classifier Accuracy:\", np.mean(y_pred == y_test[:100].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class SimpleRBM:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.W = tf.Variable(tf.random.normal([input_size, output_size], 0.01))\n",
    "        self.h_bias = tf.Variable(tf.zeros([output_size]))\n",
    "        self.v_bias = tf.Variable(tf.zeros([input_size]))\n",
    "\n",
    "    def sample(self, probs):\n",
    "        return tf.nn.relu(tf.sign(probs - tf.random.uniform(tf.shape(probs))))\n",
    "\n",
    "    def step(self, v):\n",
    "        h_probs = tf.nn.sigmoid(tf.matmul(v, self.W) + self.h_bias)\n",
    "        h_sample = self.sample(h_probs)\n",
    "        v_probs = tf.nn.sigmoid(tf.matmul(h_sample, tf.transpose(self.W)) + self.v_bias)\n",
    "        v_sample = self.sample(v_probs)\n",
    "        return h_sample, v_sample\n",
    "\n",
    "    def train(self, data, epochs=1000, lr=0.1):\n",
    "        for epoch in range(epochs):\n",
    "            for v in data:\n",
    "                v = np.reshape(v, (1, -1))\n",
    "                h_sample, v_sample = self.step(v)\n",
    "                pos_grad = tf.matmul(tf.transpose(v), h_sample)\n",
    "                neg_grad = tf.matmul(tf.transpose(v_sample), self.step(v_sample)[0])\n",
    "                self.W.assign_add(lr * (pos_grad - neg_grad))\n",
    "                self.v_bias.assign_add(lr * tf.reduce_mean(v - v_sample, axis=0))\n",
    "                self.h_bias.assign_add(lr * tf.reduce_mean(h_sample - self.step(v_sample)[0], axis=0))\n",
    "\n",
    "    def transform(self, data):\n",
    "        data = np.reshape(data, (1, -1))\n",
    "        h_probs = tf.nn.sigmoid(tf.matmul(data, self.W) + self.h_bias)\n",
    "        return h_probs\n",
    "\n",
    "\n",
    "input_size = 6\n",
    "hidden_size_1 = 3\n",
    "hidden_size_2 = 2\n",
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "data = np.array([[1, 1, 1, 0, 0, 0],\n",
    "                 [1, 0, 1, 0, 0, 0],\n",
    "                 [1, 1, 1, 0, 0, 0],\n",
    "                 [0, 0, 1, 1, 1, 0],\n",
    "                 [0, 0, 1, 1, 0, 0],\n",
    "                 [0, 0, 1, 1, 1, 0]], dtype=np.float32)\n",
    "\n",
    "rbm1 = SimpleRBM(input_size, hidden_size_1)\n",
    "rbm1.train(data, epochs, learning_rate)\n",
    "h1 = np.array([rbm1.transform(v) for v in data])\n",
    "\n",
    "rbm2 = SimpleRBM(hidden_size_1, hidden_size_2)\n",
    "rbm2.train(h1, epochs, learning_rate)\n",
    "h2 = np.array([rbm2.transform(h) for h in h1])\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Features from RBM1:\\n\", h1)\n",
    "print(\"Features from RBM2:\\n\", h2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
